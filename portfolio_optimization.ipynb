{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b19d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Projet imports\n",
    "from portfolio_environment import PortfolioEnv\n",
    "from rl_agent import PPOAgent\n",
    "from hrp_optimizer import optimizeHRP\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e32dd373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment configuration (from config.py)\n",
    "env_config = {\n",
    "'assets': config.ASSETS,\n",
    "'initial_capital': config.INITIAL_CAPITAL,\n",
    "'transaction_cost': config.TRANSACTION_COST,\n",
    "'lookback_window': config.LOOKBACK_WINDOW,\n",
    "'hrp_lookback': config.HRP_LOOKBACK,\n",
    "'start_date' : config.START_DATE,\n",
    "'end_date' : config.END_DATE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11951ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the RL agent\n",
    "def train_agent(episodes: int = 100):\n",
    "\n",
    "    env = PortfolioEnv(env_config)\n",
    "    agent = PPOAgent(config.STATE_DIM, config.ACTION_DIM, config.LEARNING_RATE)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Train agent periodically\n",
    "            if len(agent.memory) > config.BATCH_SIZE and episode % 5 == 0:\n",
    "                agent.train(config.BATCH_SIZE)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward:.2f}, \"\n",
    "                  f\"Portfolio Value: ${info['portfolio_value']:.2f}\")\n",
    "    \n",
    "    return agent, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RL agent\n",
    "agent, training_rewards = train_agent(episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both strategies\n",
    "env = PortfolioEnv(env_config)\n",
    "hrp_optimizer = HRPOptimizer()\n",
    "\n",
    "# RL strategy performance\n",
    "state = env.reset()\n",
    "rl_values = [env.initial_capital]\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(state)\n",
    "    state, _, done, info = env.step(action)\n",
    "    rl_values.append(info['portfolio_value'])\n",
    "\n",
    "# HRP strategy performance\n",
    "env.reset()\n",
    "hrp_values = [env.initial_capital]\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # Get HRP weights\n",
    "    if env.current_step >= env.hrp_lookback:\n",
    "        recent_returns = env.returns.iloc[\n",
    "            env.current_step - env.hrp_lookback:env.current_step\n",
    "        ].values\n",
    "        hrp_weights = hrp_optimizer.optimize(recent_returns)\n",
    "    else:\n",
    "        hrp_weights = np.ones(len(config.ASSETS)) / len(config.ASSETS)\n",
    "    \n",
    "    state, _, done, info = env.step(hrp_weights)\n",
    "    hrp_values.append(info['portfolio_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(training_rewards)\n",
    "plt.title('RL Training Progress')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rl_values, label='RL Strategy', alpha=0.8)\n",
    "plt.plot(hrp_values, label='HRP Baseline', alpha=0.8)\n",
    "plt.title('Portfolio Value Comparison')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Portfolio Value ($)')\n",
    "plt.legend()\n",
    "\n",
    "# Calculate performance metrics\n",
    "rl_return = (rl_values[-1] - rl_values[0]) / rl_values[0] * 100\n",
    "hrp_return = (hrp_values[-1] - hrp_values[0]) / hrp_values[0] * 100\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "strategies = ['RL Strategy', 'HRP Baseline']\n",
    "returns = [rl_return, hrp_return]\n",
    "plt.bar(strategies, returns, color=['blue', 'orange'], alpha=0.7)\n",
    "plt.title('Total Returns (%)')\n",
    "plt.ylabel('Return (%)')\n",
    "\n",
    "# Volatility comparison\n",
    "rl_volatility = np.std(np.diff(rl_values) / rl_values[:-1]) * 100\n",
    "hrp_volatility = np.std(np.diff(hrp_values) / hrp_values[:-1]) * 100\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "volatilities = [rl_volatility, hrp_volatility]\n",
    "plt.bar(strategies, volatilities, color=['blue', 'orange'], alpha=0.7)\n",
    "plt.title('Volatility (%)')\n",
    "plt.ylabel('Volatility (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('portfolio_optimization_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Performance Summary ===\")\n",
    "print(f\"RL Strategy - Return: {rl_return:.2f}%, Volatility: {rl_volatility:.2f}%\")\n",
    "print(f\"HRP Baseline - Return: {hrp_return:.2f}%, Volatility: {hrp_volatility:.2f}%\")\n",
    "print(f\"Sharpe Ratio (RL): {(rl_return / rl_volatility):.2f}\")\n",
    "print(f\"Sharpe Ratio (HRP): {(hrp_return / hrp_volatility):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
